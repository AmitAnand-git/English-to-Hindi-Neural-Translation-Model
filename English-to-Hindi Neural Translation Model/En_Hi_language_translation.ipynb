{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Checking if GPU is running or not\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KALYvZycPe8N",
        "outputId": "38037f78-470f-4f5f-df64-015e71d55178"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 27 14:47:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0              27W /  70W |   8309MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "_3XQ_jhwe0m5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "fr_hhoAFefza"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "-dbDT2O9fJEp"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q8AHfSWWMLm"
      },
      "source": [
        "## Helsinki-NLP/opus-mt-en-hi model\n",
        "\n",
        "source: https://huggingface.co/Helsinki-NLP/opus-mt-en-hi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3frhJQ_Lym"
      },
      "source": [
        "# The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXYJQHBetF"
      },
      "source": [
        "Source: https://huggingface.co/datasets/cfilt/iitb-english-hindi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "daj9JBV6jDGK"
      },
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHdy5o8zsGhm",
        "outputId": "a6113742-60f2-4aad-f871-3057e6631b26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1659083\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 520\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2507\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfaNkUHNqU11",
        "outputId": "12276ba4-af83-43c9-94fb-eb4c19f4bbf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
              "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "raw_datasets['train'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x6TtxnEEDuw"
      },
      "source": [
        "#Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "20qWg7z8qv7z"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYCILyEK0HXG",
        "outputId": "08d1bc6c-9619-4f17-9c6c-668a0ce09558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [12110, 2, 90, 23, 19, 8800, 61, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "tokenizer(\"Hello, this is a sentence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDgcI2Nz0SZN",
        "outputId": "acf0766d-68d8-4240-e3c2-cfa33fa599c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[12110, 2, 90, 23, 19, 8800, 61, 0], [239, 23, 414, 8800, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh0epnye1JJk",
        "outputId": "0fb6a12c-9e29-494d-b3eb-16909889d8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[26618, 16155, 346, 33383, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "XHaFoHKG3snJ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX9vN5Yo4kuG",
        "outputId": "4229da35-1d91-40b2-8cd1-793e454574fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[3872, 85, 2501, 132, 15441, 36398, 0], [32643, 28541, 36253, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]], 'labels': [[63, 2025, 18, 16155, 346, 20311, 24, 2279, 679, 0], [26618, 16155, 346, 33383, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "preprocess_function(raw_datasets[\"train\"][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "PTzg3eWK4_FG"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEbE41CbU1bL",
        "outputId": "502cb1cc-5596-476b-fe6a-9f8474e98aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-hi.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "GOJU15XzX5iy"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "NCoU8kNtX6Tk"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "4IbSTkjuYZ8Z"
      },
      "outputs": [],
      "source": [
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "AgvwEEvDZP7F"
      },
      "outputs": [],
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(train_dataset)\n",
        "subset_size = int(num_samples * 0.1)"
      ],
      "metadata": {
        "id": "VMrJxDAwqlcW"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "source": [
        "train_subset = train_dataset.take(subset_size)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uxqMlcPRqqfP"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "source": [
        "print(raw_datasets.shape)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pYpSLXoDpJEm",
        "outputId": "c88ba477-6448-4a10-e516-f7f5c9dde18a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': (1659083, 1), 'validation': (520, 1), 'test': (2507, 1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "E_1LNLwvZQvD"
      },
      "outputs": [],
      "source": [
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "GYF9ApeHZTR5"
      },
      "outputs": [],
      "source": [
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Wx6p1NQ_ZVXb"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5WXVp0Z15g",
        "outputId": "52920543-07ea-4cf1-8df1-e088fee54fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "10369/10369 [==============================] - 2868s 274ms/step - loss: 2.8484 - val_loss: 3.3230\n",
            "Epoch 2/15\n",
            "10369/10369 [==============================] - 2838s 274ms/step - loss: 2.5150 - val_loss: 3.1611\n",
            "Epoch 3/15\n",
            "10369/10369 [==============================] - 2863s 276ms/step - loss: 2.3757 - val_loss: 3.0655\n",
            "Epoch 4/15\n",
            " 6003/10369 [================>.............] - ETA: 20:02 - loss: 2.3121"
          ]
        }
      ],
      "source": [
        "model.fit(train_subset, validation_data=validation_dataset, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "4EWkhFnyZ3Z8"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcGNz4yrMY4F"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTkqZ2fuLbd1",
        "outputId": "e0c7316e-af98-44c7-a6d0-c8e7e5b35132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at tf_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4-NztUuM6pk",
        "outputId": "38b66aed-08be-4ce0-9a4c-09ec3cc7da95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[61949   459     2    60   217  3176     6   116  1001  7350  1054   130\n",
            "   5029  8653   361 10430     5     0]], shape=(1, 18), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "input_text  = \"Hey, this is Ajay Kumar and team, with our project  \"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OIrGT1uOjKe",
        "outputId": "cbf9035f-9624-4dbc-d8fc-5bafd9d8b71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ऐ, यह हमारे प्रोजेक्ट के साथ अंज सची कुमार व टीम है\n"
          ]
        }
      ],
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WjVcYschmbn"
      },
      "execution_count": 96,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}